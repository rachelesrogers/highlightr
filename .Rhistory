col_descript_long$rel_freq <- col_descript_long$final_count/col_descript_long$transcript_freq
descript_tomerge <- col_descript_long %>% dplyr::select(rel_freq, col_number, word_number) %>%
tidyr::pivot_wider(names_from = col_number, values_from = rel_freq, names_prefix = "col_")
add_word<-descript_ngram_df %>% dplyr::select(word_1, first_word, collocation) %>%
dplyr::rename("word_number"="word_1")
descript_tomerge <- dplyr::left_join(descript_tomerge, add_word)
descript_tomerge<-descript_tomerge %>% dplyr::rename("to_merge"="first_word")
for (i in 2:collocate_length){
descript_tomerge[dim(descript_tomerge)[1]-(collocate_length-i),]$to_merge <-
stringr::word(descript_tomerge[dim(descript_tomerge)[1]-(collocate_length-1),]$collocation, i)
}
descript_tomerge
devtools::document()
import(zoomerjoin)
use_package(zoomerjoin)
use_package("zoomerjoin")
check()
pkgbuild::check_build_tools(debug = TRUE)
check()
pkgbuild::check_build_tools(debug = TRUE)
check()
dash_test <-
data.frame(ID=1:6,
Notes=c("dash-name did this", "year 1892-1777 was significant", "another dash-name did this",
"in year 1892-1777 dash-name did another thing", "in an example - here is a dash space",
"in an example - here is a dash space with dash-name and year 1892-1777"))
comment_example_rename <- dplyr::rename(dash_test, page_notes=Notes)
toks_comment <- token_comments(comment_example_rename)
dash_transcript <- data.frame(Text="in an example - here is a dash space
in the year 1892-1777 dash-name did this")
transcript_example_rename <- dplyr::rename(dash_transcript, text=Text)
toks_transcript <- token_transcript(transcript_example_rename)
collocation_object <- collocate_comments_fuzzy(toks_transcript, toks_comment, collocate_length = 2)
transcript_token<- toks_transcript
note_token <- toks_comment
collocate_length <- 2
collocation.y <- dist <- collocation.x <- weighted_count <- col_number <- word_number <-
word_1 <- first_word <- collocation <- NULL
`%>%` <- magrittr::`%>%`
#Same as previous notes
descript_ngrams <- quanteda::tokens_ngrams(transcript_token, n = collocate_length, skip = 0L, concatenator = " ")
descript_ngram_df <- data.frame(unlist(descript_ngrams))
rel_freq <-as.data.frame(table(descript_ngram_df))
descript_ngram_df <- dplyr::left_join(descript_ngram_df, rel_freq)
names(descript_ngram_df) <- c("collocation", "transcript_freq")
descript_ngram_df <-data.frame(collocation = descript_ngram_df$collocation,
transcript_freq = descript_ngram_df$transcript_freq)
for (i in 1:collocate_length){
descript_ngram_df <- cbind(descript_ngram_df, seq(from=i, to = dim(descript_ngram_df)[1]+(i-1)))
names(descript_ngram_df)[ncol(descript_ngram_df)]<-paste0("word_",i)
}
descript_ngram_df$first_word <- stringr::word(descript_ngram_df$collocation,1)
col_descript <- note_token %>% quanteda.textstats::textstat_collocations(min_count = 1,
size=collocate_length)
col_merged_descript <- dplyr::left_join(descript_ngram_df, col_descript)
col_merged_descript$count <- replace(col_merged_descript$count,is.na(col_merged_descript$count),0)
# Finding collocations that do not directly match the transcript
mismatches <- dplyr::anti_join(col_descript, descript_ngram_df)
mismatches
descript_ngram_df
fuzzy_matches <- zoomerjoin::jaccard_right_join(descript_ngram_df, mismatches,
by='collocation', similarity_column="dist", n_bands=2000, threshold=0.5)%>%
dplyr::filter(!is.na(collocation.x)) %>%
dplyr::group_by(collocation.y) %>%
dplyr::slice_max(order_by=dist, n=1) #finding closest match based on Jaccard Distance
fuzzy_matches
dash_test
is.na(fuzzy_matches)
dim(fuzzy_matches)
fuzzy_matches <- zoomerjoin::jaccard_right_join(descript_ngram_df, mismatches,
by='collocation', similarity_column="dist", n_bands=2000, threshold=0.1)%>%
dplyr::filter(!is.na(collocation.x)) %>%
dplyr::group_by(collocation.y) %>%
dplyr::slice_max(order_by=dist, n=1) #finding closest match based on Jaccard Distance
dash_test
dash_transcript
mismatches
descript_ngram_df
fuzzy_matches <- zoomerjoin::jaccard_right_join(descript_ngram_df, mismatches,
by='collocation', similarity_column="dist", n_bands=2000, threshold=0.25)%>%
dplyr::filter(!is.na(collocation.x)) %>%
dplyr::group_by(collocation.y) %>%
dplyr::slice_max(order_by=dist, n=1) #finding closest match based on Jaccard Distance
fuzzy_matches <- zoomerjoin::jaccard_right_join(descript_ngram_df, mismatches,
by='collocation', similarity_column="dist", n_bands=20000, threshold=0.25)%>%
dplyr::filter(!is.na(collocation.x)) %>%
dplyr::group_by(collocation.y) %>%
dplyr::slice_max(order_by=dist, n=1) #finding closest match based on Jaccard Distance
fuzzy_matches <- zoomerjoin::jaccard_right_join(descript_ngram_df, mismatches,
by='collocation', similarity_column="dist", n_bands=20000, threshold=0.4)%>%
dplyr::filter(!is.na(collocation.x)) %>%
dplyr::group_by(collocation.y) %>%
dplyr::slice_max(order_by=dist, n=1) #finding closest match based on Jaccard Distance
fuzzy_matches
load_all()
collocation_object <- collocate_comments_fuzzy(toks_transcript, toks_comment, collocate_length = 2)
frequency_test <- transcript_frequency(transcript_example_rename, collocation_object)
expect_identical(frequency_test$to_merge, c("in","an","example","","here","is","a","dash","space","in",
"the","year","1892","","1777","dash","","name","did","this"))
frequency_test
check()
rep_test <-
data.frame(ID=1:6,
Notes=c(rep("in an example", 3)))
rep_test
rep_test <-
data.frame(ID=1:6,
Notes=c(rep("in an example", 6)))
comment_example_rename <- dplyr::rename(rep_test, page_notes=Notes)
toks_comment <- token_comments(comment_example_rename)
dash_transcript <- data.frame(Text="in an example - here is a dash space
in the year 1892-1777 dash-name did this")
transcript_example_rename <- dplyr::rename(dash_transcript, text=Text)
toks_transcript <- token_transcript(transcript_example_rename)
collocation_object <- collocate_comments_fuzzy(toks_transcript, toks_comment, collocate_length = 2)
dim(fuzzy_matches)
fuzzy_matches
load_all()
dash_test <-
data.frame(ID=1:6,
Notes=c("dash-name did this", "year 1892-1777 was significant", "another dash-name did this",
"in year 1892-1777 dash-name did another thing", "in an example - here is a dash space",
"in an example - here is a dash space with dash-name and year 1892-1777"))
comment_example_rename <- dplyr::rename(dash_test, page_notes=Notes)
toks_comment <- token_comments(comment_example_rename)
dash_transcript <- data.frame(Text="in an example - here is a dash space
in the year 1892-1777 dash-name did this")
transcript_example_rename <- dplyr::rename(dash_transcript, text=Text)
toks_transcript <- token_transcript(transcript_example_rename)
collocation_object <- collocate_comments_fuzzy(toks_transcript, toks_comment, collocate_length = 2)
rep_test <-
data.frame(ID=1:6,
Notes=c(rep("in an example", 6)))
comment_example_rename <- dplyr::rename(rep_test, page_notes=Notes)
toks_comment <- token_comments(comment_example_rename)
dash_transcript <- data.frame(Text="in an example - here is a dash space
in the year 1892-1777 dash-name did this")
transcript_example_rename <- dplyr::rename(dash_transcript, text=Text)
toks_transcript <- token_transcript(transcript_example_rename)
collocation_object <- collocate_comments_fuzzy(toks_transcript, toks_comment, collocate_length = 2)
collocation_object
frequency_test <- transcript_frequency(transcript_example_rename, collocation_object)
frequency_test
frequency_test$Freq[1:3]
frequency_test$Freq[1:3]==c(6,6,3)
check()
check()
devtools::build_rmd("vignettes/highlightr.Rmd")
load_all()
comment_example_rename <- dplyr::rename(comment_example, page_notes=Notes)
toks_comment <- token_comments(comment_example_rename)
transcript_example_rename <- dplyr::rename(transcript_example, text=Text)
toks_transcript <- token_transcript(transcript_example_rename)
collocation_object <- collocate_comments_fuzzy(toks_transcript, toks_comment)
collocation_object <- collocate_comments_fuzzy(toks_transcript, toks_comment)
head(collocation_object)
merged_frequency <- transcript_frequency(transcript_example_rename, collocation_object)
View(merged_frequency)
View(collocation_object)
transcript_token <- toks_transcript
note_token <- toks_comment
#Same as previous notes
descript_ngrams <- quanteda::tokens_ngrams(transcript_token, n = collocate_length, skip = 0L, concatenator = " ")
descript_ngram_df <- data.frame(unlist(descript_ngrams))
rel_freq <-as.data.frame(table(descript_ngram_df))
descript_ngram_df <- dplyr::left_join(descript_ngram_df, rel_freq)
names(descript_ngram_df) <- c("collocation", "transcript_freq")
descript_ngram_df <-data.frame(collocation = descript_ngram_df$collocation,
transcript_freq = descript_ngram_df$transcript_freq)
for (i in 1:collocate_length){
descript_ngram_df <- cbind(descript_ngram_df, seq(from=i, to = dim(descript_ngram_df)[1]+(i-1)))
names(descript_ngram_df)[ncol(descript_ngram_df)]<-paste0("word_",i)
}
descript_ngram_df$first_word <- stringr::word(descript_ngram_df$collocation,1)
col_descript <- note_token %>% quanteda.textstats::textstat_collocations(min_count = 1,
size=collocate_length)
col_merged_descript <- dplyr::left_join(descript_ngram_df, col_descript)
col_merged_descript$count <- replace(col_merged_descript$count,is.na(col_merged_descript$count),0)
# Finding collocations that do not directly match the transcript
mismatches <- dplyr::anti_join(col_descript, descript_ngram_df)
View(mismatches)
View(fuzzy_matches)
View(descript_ngram_df)
fuzzy_matches <- zoomerjoin::jaccard_right_join(descript_ngram_df, mismatches,
by='collocation', similarity_column="dist", n_bands=20000, threshold=0.4)%>%
dplyr::filter(!is.na(collocation.x)) %>%
dplyr::group_by(collocation.y) %>%
dplyr::slice_max(order_by=dist, n=1) #finding closest match based on Jaccard Distance
View(fuzzy_matches)
close_freq<-as.data.frame(table(fuzzy_matches$collocation.y))
close_freq <- close_freq %>% dplyr::rename("collocation.y"="Var1", "close_freq"="Freq")
View(close_freq)
fuzzy_matches <- dplyr::left_join(fuzzy_matches, close_freq)
View(fuzzy_matches)
# Finding collocations that do not directly match the transcript
mismatches <- dplyr::anti_join(col_descript, descript_ngram_df)
View(mismatches)
View(col_descript)
View(note_token)
check()
load_all()
toks_comment <- token_comments(highlightr::wiki_pages)
transcript_example_rename <- data.frame(text=wiki_pages[1,])
toks_transcript <- token_transcript(transcript_example_rename)
collocation_object <- collocate_comments_fuzzy(toks_transcript, toks_comment)
collocation_object <- collocate_comments_fuzzy(toks_transcript, toks_comment)
head(collocation_object)
collocation_object <- collocate_comments_fuzzy(toks_transcript, toks_comment)
collocation_object2 <- collocate_comments(toks_transcript, toks_comment)
head(collocation_object2)
transcript_example_rename2 <- data.frame(text=wiki_pages[dim(wiki_pages)[1],])
toks_transcript2 <- token_transcript(transcript_example_rename2)
collocation_object2 <- collocate_comments_fuzzy(toks_transcript2, toks_comment)
collocation_object2 <- collocate_comments_fuzzy(toks_transcript2, toks_comment)
merged_frequency2 <- transcript_frequency(transcript_example_rename2, collocation_object2)
merged_frequency2
min(merged_frequency2$Freq)
View(merged_frequency2)
freq_plot2 <- collocation_plot(merged_frequency2)
page_highlight2 <- highlighted_text(freq_plot2, labels=c("(fewest articles)", "(most articles)"))
View(page_highlight2)
page_highlight2
freq_plot2$freq
View(freq_plot2$freq)
comment_example_rename <- dplyr::rename(comment_example, page_notes=Notes)
toks_comment <- token_comments(comment_example_rename)
transcript_example_rename <- dplyr::rename(transcript_example, text=Text)
toks_transcript <- token_transcript(transcript_example_rename)
collocation_object <- collocate_comments_fuzzy(toks_transcript, toks_comment)
View(collocation_object)
merged_frequency <- transcript_frequency(transcript_example_rename, collocation_object)
View(merged_frequency)
check()
use_release_issue()
urlchecker::url_check()
devtools::build_readme()
devtools::check(remote = TRUE, manual = TRUE)
---
output: github_document
library(tinytex)
tl_pkgs()
tinytex::parse_install()
tinytex::parse_install(text = "`makeindex` not found")
tinytex::parse_install(text = "! LaTeX Error: File `makeidx' not found.")
devtools::check(remote = TRUE, manual = TRUE)
devtools::build_manual()
tinytex::tlmgr_install("makeindex")
devtools::build_manual()
devtools::check(remote = TRUE, manual = TRUE)
comment_example_rename <- dplyr::rename(comment_example, page_notes=Notes)
toks_comment <- token_comments(comment_example_rename)
transcript_example_rename <- dplyr::rename(transcript_example, text=Text)
toks_transcript <- token_transcript(transcript_example_rename)
load_all()
collocation_object <- collocate_comments_fuzzy(toks_transcript, toks_comment)
transcript_token<- toks_transcript
note_token <- toks_comment
collocate_length = 5
#Same as previous notes
descript_ngrams <- quanteda::tokens_ngrams(transcript_token, n = collocate_length, skip = 0L, concatenator = " ")
descript_ngram_df <- data.frame(unlist(descript_ngrams))
rel_freq <-as.data.frame(table(descript_ngram_df))
descript_ngram_df <- dplyr::left_join(descript_ngram_df, rel_freq)
names(descript_ngram_df) <- c("collocation", "transcript_freq")
descript_ngram_df <-data.frame(collocation = descript_ngram_df$collocation,
transcript_freq = descript_ngram_df$transcript_freq)
for (i in 1:collocate_length){
descript_ngram_df <- cbind(descript_ngram_df, seq(from=i, to = dim(descript_ngram_df)[1]+(i-1)))
names(descript_ngram_df)[ncol(descript_ngram_df)]<-paste0("word_",i)
}
descript_ngram_df$first_word <- stringr::word(descript_ngram_df$collocation,1)
col_descript <- note_token %>% quanteda.textstats::textstat_collocations(min_count = 1,
size=collocate_length)
col_merged_descript <- dplyr::left_join(descript_ngram_df, col_descript)
col_merged_descript$count <- replace(col_merged_descript$count,is.na(col_merged_descript$count),0)
# Finding collocations that do not directly match the transcript
mismatches <- dplyr::anti_join(col_descript, descript_ngram_df)
fuzzy_matches <- zoomerjoin::jaccard_right_join(descript_ngram_df, mismatches,
by='collocation', similarity_column="dist", n_bands=10000, threshold=0.4)%>%
dplyr::filter(!is.na(collocation.x)) %>%
dplyr::group_by(collocation.y) %>%
dplyr::slice_max(order_by=dist, n=1) #finding closest match based on Jaccard Distance
check()
fuzzy_matches <- zoomerjoin::jaccard_right_join(descript_ngram_df, mismatches,
by='collocation', similarity_column="dist", n_bands=5000, threshold=0.4)%>%
dplyr::filter(!is.na(collocation.x)) %>%
dplyr::group_by(collocation.y) %>%
dplyr::slice_max(order_by=dist, n=1) #finding closest match based on Jaccard Distance
check()
document()
devtools::build_readme()
devtools::check(remote = TRUE, manual = TRUE)
devtools::check_win_devel()
View(comment_example_rename)
View(comment_example_rename[1:50])
View(comment_example_rename[1:50,])
check()
document()
revdepcheck::revdep_check(num_workers = 4)
install.packages("revdepcheck")
revdepcheck::revdep_check(num_workers = 4)
devtools::install_github("r-lib/revdepcheck")
revdepcheck::revdep_check(num_workers = 4)
detach(highlightr)
detach("highlightr")
revdepcheck::revdep_check(num_workers = 4)
devtools::build_readme()
devtools::check(remote = TRUE, manual = TRUE)
devtools::check_win_devel()
library(devtools)
document()
document
document()
pkgload::dev_help('collocate_comments_fuzzy')
load_all()
toks_comment <- token_comments(comment_example_rename)
comment_example_rename <- dplyr::rename(comment_example, page_notes=Notes)
toks_comment <- token_comments(comment_example_rename)
transcript_example_rename <- dplyr::rename(transcript_example, text=Text)
toks_transcript <- token_transcript(transcript_example_rename)
collocation_object <- collocate_comments_fuzzy(toks_transcript, toks_comment, n_bands=40, threshold=0.9)
collocation_object
collocation_object2 <- collocate_comments(toks_transcript, toks_comment)
collocation_object2
build_readme()
devtools::build_rmd("vignettes/highlightr.Rmd")
document()
check()
check()
urlchecker::url_check()
devtools::build_readme()
devtools::check(remote = TRUE, manual = TRUE)
devtools::check(remote = TRUE, manual = TRUE)
devtools::check_win_devel()
document()
check()
devtools::build_readme()
devtools::check(remote = TRUE, manual = TRUE)
devtools::check_win_devel()
collocate_comments_fuzzy(toks_transcript, toks_comment, n_bands=40, threshold=0.9)
load_all()
start_time <- proc.time()
comment_example_rename <- dplyr::rename(comment_example, page_notes=Notes)
toks_comment <- token_comments(comment_example_rename)
transcript_example_rename <- dplyr::rename(transcript_example, text=Text)
toks_transcript <- token_transcript(transcript_example_rename)
fuzzy_object <- collocate_comments_fuzzy(toks_transcript, toks_comment, n_bands=40, threshold=0.9)
end_time <- proc.time()
start_time - end_time
end_time - start_time
comment_example_rename
comment_example_rename <- dplyr::rename(comment_example[1:10,], page_notes=Notes)
toks_comment <- token_comments(comment_example_rename)
transcript_example_rename <- dplyr::rename(transcript_example, text=Text)
toks_transcript <- token_transcript(transcript_example_rename)
fuzzy_object <- collocate_comments_fuzzy(toks_transcript, toks_comment, n_bands=40, threshold=0.9)
fuzzy_object
start_time <- proc.time()
comment_example_rename <- dplyr::rename(comment_example[1:10,], page_notes=Notes)
toks_comment <- token_comments(comment_example_rename)
transcript_example_rename <- dplyr::rename(transcript_example, text=Text)
toks_transcript <- token_transcript(transcript_example_rename)
fuzzy_object <- collocate_comments_fuzzy(toks_transcript, toks_comment, n_bands=40, threshold=0.9)
end_time <- proc.time()
end_time - start_time
document()
check()
devtools::build_readme()
devtools::check(remote = TRUE, manual = TRUE)
devtools::check_win_devel()
load_all()
comment_example_rename <- dplyr::rename(comment_example[1:10,], page_notes=Notes)
toks_comment <- token_comments(comment_example_rename)
transcript_example_rename <- dplyr::rename(transcript_example, text=Text)
toks_transcript <- token_transcript(transcript_example_rename)
fuzzy_object <- collocate_comments_fuzzy(toks_transcript, toks_comment, n_bands=40, threshold=0.9)
fuzzy_object
fuzzy_object <- collocate_comments_fuzzy(toks_transcript, toks_comment, n_bands=40, threshold=0.95)
fuzzy_object
fuzzy_object <- collocate_comments_fuzzy(toks_transcript, toks_comment, n_bands=10, threshold=0.95)
fuzzy_object
fuzzy_object <- collocate_comments_fuzzy(toks_transcript, toks_comment, n_bands=10, threshold=1)
fuzzy_object
check()
devtools::check(remote = TRUE, manual = TRUE)
devtools::check_win_devel()
devtools::check_win_devel()
library(rhub)
install.packages("rhub")
library(rhub)
rhub_platforms()
rhub_check()
usethis::use_version('patch')
devtools::submit_cran()
library(highlightr)
toks_comment <- token_comments(highlightr::wiki_pages)
library(rvest)
class_of_interest <- ".mw-content-ltr" ## ids are #id-name, classes are .class-name
editurl <- "https://en.wikipedia.org/w/index.php?title=Highlighter&action=history&offset=&limit=150"
editclass_of_interest <- ".mw-changeslist-date"
url_list1 <- editurl %>%
rvest::read_html() %>%
rvest::html_nodes(editclass_of_interest) %>%
map(., list()) %>%
tibble(node = .) %>%
mutate(link = map_chr(node, html_attr, "href") %>% paste0("https://en.wikipedia.org", .))
url_list1 <- editurl %>%
rvest::read_html() %>%
rvest::html_nodes(editclass_of_interest) %>%
map(., list()) %>%
tibble(node = .) %>%
dplyr::mutate(link = purrr::map_chr(node, html_attr, "href") %>% paste0("https://en.wikipedia.org", .))
library(rvest)
library(dplyr)
library(purrr)
library(tibble)
class_of_interest <- ".mw-content-ltr" ## ids are #id-name, classes are .class-name
editurl <- "https://en.wikipedia.org/w/index.php?title=Highlighter&action=history&offset=&limit=150"
editclass_of_interest <- ".mw-changeslist-date"
url_list1 <- editurl %>%
rvest::read_html() %>%
rvest::html_nodes(editclass_of_interest) %>%
map(., list()) %>%
tibble::tibble(node = .) %>%
dplyr::mutate(link = purrr::map_chr(node, html_attr, "href") %>% paste0("https://en.wikipedia.org", .))
editurl2 <- "https://en.wikipedia.org/w/index.php?title=Highlighter&action=history&dir=prev&limit=150"
url_list2 <- editurl2 %>%
rvest::read_html() %>%
rvest::html_nodes(editclass_of_interest) %>%
purrr::map(., list()) %>%
tibble::tibble(node = .) %>%
dplyr::mutate(link = purrr::map_chr(node, html_attr, "href") %>% paste0("https://en.wikipedia.org", .))
url_list <- rbind(url_list1, url_list2)
wiki_pages <- data.frame(page_notes = rep(NA, dim(url_list)[1]))
for (i in 1:dim(url_list)[1]){
wiki_list <-  url_list$link[i] %>%
rvest::read_html() %>%
rvest::html_node(class_of_interest) %>%
rvest::html_children() %>%
purrr::map(., list()) %>%
tibble::tibble(node = .) %>%
dplyr::mutate(type = purrr::map_chr(node, html_name)) %>%
dplyr::filter(type == "p") %>%
dplyr::mutate(text = purrr::map_chr(node, html_text)) %>%
dplyr::mutate(cleantext = stringr::str_remove_all(text, "\\[.*?\\]") %>% str_trim()) %>%
plyr::summarise(cleantext = paste(cleantext, collapse = "<br> "))
wiki_pages$page_notes[i] <- wiki_list$cleantext[1]
}
for (i in 1:dim(url_list)[1]){
wiki_list <-  url_list$link[i] %>%
rvest::read_html() %>%
rvest::html_node(class_of_interest) %>%
rvest::html_children() %>%
purrr::map(., list()) %>%
tibble::tibble(node = .) %>%
dplyr::mutate(type = purrr::map_chr(node, html_name)) %>%
dplyr::filter(type == "p") %>%
dplyr::mutate(text = purrr::map_chr(node, html_text)) %>%
dplyr::mutate(cleantext = stringr::str_remove_all(text, "\\[.*?\\]") %>% stringr::str_trim()) %>%
plyr::summarise(cleantext = paste(cleantext, collapse = "<br> "))
wiki_pages$page_notes[i] <- wiki_list$cleantext[1]
}
View(wiki_pages)
library(devtools)
check()
check()
check()
devtools::load_all()
?highlighted_text
document()
devtools::load_all()
?highlighted_text
document()
devtools::load_all()
?highlighted_text
document()
devtools::load_all()
?highlighted_text
document()
devtools::load_all()
?highlighted_text
document()
devtools::load_all()
?highlighted_text
check()
load_all()
toks_comment <- token_comments(highlightr::wiki_pages)
transcript_example_rename <- data.frame(text=wiki_pages[1,])
toks_transcript <- token_transcript(transcript_example_rename)
collocation_object <- collocate_comments_fuzzy(toks_transcript, toks_comment)
head(collocation_object)
merged_frequency <- transcript_frequency(transcript_example_rename, collocation_object)
freq_plot <- collocation_plot(merged_frequency)
page_highlight <- highlighted_text(freq_plot, labels=c("(fewest articles)", "(most articles)"))
library(xml2)
write_html(page_highlight, "test.html")
install.packages("textutils")
library(textutils)
toHTML(page_highlight)
write_html(read_html(page_highlight), "test.html")
read_html(page_highlight)
load_all()
document()
load_all()
?highlighted_text
document()
check()
check()
library(devtools)
check()
check()
check()
