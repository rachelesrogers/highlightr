wiki_pages
data.frame(observation = 1:dim(url_list)[1], Notes = wiki_pages)
View(data.frame(observation = 1:dim(url_list)[1], Notes = wiki_pages))
View(wiki_pages)
View(data.frame(observation = 1:dim(url_list)[1], Notes = wiki_pages$Value))
url_list$link[i] %>%
read_html() %>%
html_node(class_of_interest) %>%
html_children() %>%
map(., list()) %>%
tibble(node = .) %>%
mutate(type = map_chr(node, html_name)) %>%
filter(type == "p") %>%
mutate(text = map_chr(node, html_text)) %>%
mutate(cleantext = str_remove_all(text, "\\[.*?\\]") %>% str_trim()) %>%
plyr::summarise(cleantext = paste(cleantext, collapse = " "))
wiki_pages[[1]]
View(wik_pages[1])
View(wiki_pages[1])
wiki_list <-  url_list$link[i] %>%
read_html() %>%
html_node(class_of_interest) %>%
html_children() %>%
map(., list()) %>%
tibble(node = .) %>%
mutate(type = map_chr(node, html_name)) %>%
filter(type == "p") %>%
mutate(text = map_chr(node, html_text)) %>%
mutate(cleantext = str_remove_all(text, "\\[.*?\\]") %>% str_trim()) %>%
plyr::summarise(cleantext = paste(cleantext, collapse = " "))
wiki_list
View(wiki_list)
View(wiki_list[1])
wiki_list <-  url_list$link[i] %>%
read_html() %>%
html_node(class_of_interest) %>%
html_children() %>%
map(., list()) %>%
tibble(node = .) %>%
mutate(type = map_chr(node, html_name)) %>%
filter(type == "p") %>%
mutate(text = map_chr(node, html_text)) %>%
mutate(cleantext = str_remove_all(text, "\\[.*?\\]") %>% str_trim()) %>%
plyr::summarise(paste(cleantext, collapse = " "))
wiki_pages <- list(rep(NA, dim(url_list)[1]))
for (i in 1:dim(url_list)[1]){
wiki_list <-  url_list$link[i] %>%
read_html() %>%
html_node(class_of_interest) %>%
html_children() %>%
map(., list()) %>%
tibble(node = .) %>%
mutate(type = map_chr(node, html_name)) %>%
filter(type == "p") %>%
mutate(text = map_chr(node, html_text)) %>%
mutate(cleantext = str_remove_all(text, "\\[.*?\\]") %>% str_trim()) %>%
plyr::summarise(cleantext = paste(cleantext, collapse = " "))
wiki_pages[i] <- wiki_list$cleantext
}
View(wiki_pages)
for (i in 1:dim(url_list)[1]){
wiki_list <-  url_list$link[i] %>%
read_html() %>%
html_node(class_of_interest) %>%
html_children() %>%
map(., list()) %>%
tibble(node = .) %>%
mutate(type = map_chr(node, html_name)) %>%
filter(type == "p") %>%
mutate(text = map_chr(node, html_text)) %>%
mutate(cleantext = str_remove_all(text, "\\[.*?\\]") %>% str_trim()) %>%
plyr::summarise(cleantext = paste(cleantext, collapse = " "))
wiki_pages[i] <- wiki_list$cleantext[1]
}
View(wiki_pages)
wiki_pages <- data.frame(rep(NA, dim(url_list)[1]))
wiki_pages <- data.frame(Notes = rep(NA, dim(url_list)[1]))
wiki_pages[i]
wiki_pages$Notes[i]
for (i in 1:dim(url_list)[1]){
wiki_list <-  url_list$link[i] %>%
read_html() %>%
html_node(class_of_interest) %>%
html_children() %>%
map(., list()) %>%
tibble(node = .) %>%
mutate(type = map_chr(node, html_name)) %>%
filter(type == "p") %>%
mutate(text = map_chr(node, html_text)) %>%
mutate(cleantext = str_remove_all(text, "\\[.*?\\]") %>% str_trim()) %>%
plyr::summarise(cleantext = paste(cleantext, collapse = " "))
wiki_pages$Notes[i] <- wiki_list$cleantext[1]
}
View(wiki_pages)
toks_comment <- token_comments(comment_example_rename)
toks_comment <- token_comments(wiki_pages)
wiki_pages <- data.frame(page_notes = rep(NA, dim(url_list)[1]))
for (i in 1:dim(url_list)[1]){
wiki_list <-  url_list$link[i] %>%
read_html() %>%
html_node(class_of_interest) %>%
html_children() %>%
map(., list()) %>%
tibble(node = .) %>%
mutate(type = map_chr(node, html_name)) %>%
filter(type == "p") %>%
mutate(text = map_chr(node, html_text)) %>%
mutate(cleantext = str_remove_all(text, "\\[.*?\\]") %>% str_trim()) %>%
plyr::summarise(cleantext = paste(cleantext, collapse = " "))
wiki_pages$page_notes[i] <- wiki_list$cleantext[1]
}
View(wiki_pages)
toks_comment <- token_comments(wiki_pages)
first_dataset <- url %>%
read_html() %>%
html_node(class_of_interest) %>%
html_children() %>%
map(., list()) %>%
tibble(node = .) %>%
mutate(type = map_chr(node, html_name)) %>%
filter(type == "p") %>%
mutate(text = map_chr(node, html_text)) %>%
mutate(cleantext = str_remove_all(text, "\\[.*?\\]") %>% str_trim()) %>%
plyr::summarise(cleantext = paste(cleantext, collapse = " "))
View(first_dataset)
transcript_example_rename <- dplyr::rename(first_dataset, cleantext=text)
transcript_example_rename <- dplyr::rename(first_dataset, text=cleantext)
toks_transcript <- token_transcript(transcript_example_rename)
collocation_object <- collocate_comments_fuzzy(toks_transcript, toks_comment)
head(collocation_object)
merged_frequency <- transcript_frequency(transcript_example_rename, collocation_object)
freq_plot <- collocation_plot(merged_frequency)
page_highlight <- highlighted_text(freq_plot, merged_frequency)
page_highlight
adist(first_dataset$cleantext, wiki_pages$page_notes[1])
wiki_pages[1,]
View(wiki_pages[1,])
transcript_example_rename <- dplyr::rename(wiki_pages$page_notes[1], text=cleantext)
transcript_example_rename <- dplyr::rename(wiki_pages[1,], text=cleantext)
View(data.frame(wiki_pages[1,]))
View(data.frame(text=wiki_pages[1,]))
View(data.frame(text=wiki_pages[1]))
transcript_example_rename <- data.frame(text=wiki_pages[1,])
toks_transcript <- token_transcript(transcript_example_rename)
collocation_object <- collocate_comments_fuzzy(toks_transcript, toks_comment)
head(collocation_object)
merged_frequency <- transcript_frequency(transcript_example_rename, collocation_object)
freq_plot <- collocation_plot(merged_frequency)
page_highlight <- highlighted_text(freq_plot, merged_frequency)
page_highlight
usethis::use_data(wiki_pages)
usethis::use_data_raw("wiki_pages")
editurl <- "https://en.wikipedia.org/w/index.php?title=Highlighter&action=history"
editclass_of_interest <- ".mw-changeslist-date"
url_list <- editurl %>%
read_html() %>%
html_nodes(editclass_of_interest) %>%
map(., list()) %>%
tibble(node = .) %>%
mutate(link = map_chr(node, html_attr, "href") %>% paste0("https://en.wikipedia.org", .))
wiki_pages <- data.frame(page_notes = rep(NA, dim(url_list)[1]))
for (i in 1:dim(url_list)[1]){
wiki_list <-  url_list$link[i] %>%
read_html() %>%
html_node(class_of_interest) %>%
html_children() %>%
map(., list()) %>%
tibble(node = .) %>%
mutate(type = map_chr(node, html_name)) %>%
filter(type == "p") %>%
mutate(text = map_chr(node, html_text)) %>%
mutate(cleantext = str_remove_all(text, "\\[.*?\\]") %>% str_trim()) %>%
plyr::summarise(cleantext = paste(cleantext, collapse = " "))
wiki_pages$page_notes[i] <- wiki_list$cleantext[1]
}
usethis::use_data(wiki_pages, overwrite = TRUE)
dim(wiki_pages)
names(wiki_pages)
dim(transcript_example)
View(transcript_example)
dim(comment_example)
names(comment_example)
usethis::use_vignette("wikipedia-highlighter")
load_all()
load("~/PhD Research/highlightr/data/wiki_pages.rda")
library(highlightr)
load(wiki_pages)
devtools::install()
class_of_interest <- ".mw-content-ltr" ## ids are #id-name, classes are .class-name
editurl <- "https://en.wikipedia.org/w/index.php?title=Highlighter&action=history"
editclass_of_interest <- ".mw-changeslist-date"
url_list <- editurl %>%
read_html() %>%
html_nodes(editclass_of_interest) %>%
map(., list()) %>%
tibble(node = .) %>%
mutate(link = map_chr(node, html_attr, "href") %>% paste0("https://en.wikipedia.org", .))
wiki_pages <- data.frame(page_notes = rep(NA, dim(url_list)[1]))
for (i in 1:dim(url_list)[1]){
wiki_list <-  url_list$link[i] %>%
read_html() %>%
html_node(class_of_interest) %>%
html_children() %>%
map(., list()) %>%
tibble(node = .) %>%
mutate(type = map_chr(node, html_name)) %>%
filter(type == "p") %>%
mutate(text = map_chr(node, html_text)) %>%
mutate(cleantext = str_remove_all(text, "\\[.*?\\]") %>% str_trim()) %>%
plyr::summarise(cleantext = paste(cleantext, collapse = "\n"))
wiki_pages$page_notes[i] <- wiki_list$cleantext[1]
}
usethis::use_data(wiki_pages, overwrite = TRUE)
devtools::install()
View(wiki_pages)
View(wiki_pages)
devtools::build_rmd("vignettes/wikipedia-highlighter.Rmd")
devtools::build_rmd("vignettes/wikipedia-highlighter.Rmd")
class_of_interest <- ".mw-content-ltr" ## ids are #id-name, classes are .class-name
editurl <- "https://en.wikipedia.org/w/index.php?title=Highlighter&action=history&offset=&limit=500"
editclass_of_interest <- ".mw-changeslist-date"
url_list <- editurl %>%
read_html() %>%
html_nodes(editclass_of_interest) %>%
map(., list()) %>%
tibble(node = .) %>%
mutate(link = map_chr(node, html_attr, "href") %>% paste0("https://en.wikipedia.org", .))
wiki_pages <- data.frame(page_notes = rep(NA, dim(url_list)[1]))
for (i in 1:dim(url_list)[1]){
wiki_list <-  url_list$link[i] %>%
read_html() %>%
html_node(class_of_interest) %>%
html_children() %>%
map(., list()) %>%
tibble(node = .) %>%
mutate(type = map_chr(node, html_name)) %>%
filter(type == "p") %>%
mutate(text = map_chr(node, html_text)) %>%
mutate(cleantext = str_remove_all(text, "\\[.*?\\]") %>% str_trim()) %>%
plyr::summarise(cleantext = paste(cleantext, collapse = "\n"))
wiki_pages$page_notes[i] <- wiki_list$cleantext[1]
}
class_of_interest <- ".mw-content-ltr" ## ids are #id-name, classes are .class-name
editurl <- "https://en.wikipedia.org/w/index.php?title=Highlighter&action=history&offset=&limit=250"
editclass_of_interest <- ".mw-changeslist-date"
url_list <- editurl %>%
read_html() %>%
html_nodes(editclass_of_interest) %>%
map(., list()) %>%
tibble(node = .) %>%
mutate(link = map_chr(node, html_attr, "href") %>% paste0("https://en.wikipedia.org", .))
wiki_pages <- data.frame(page_notes = rep(NA, dim(url_list)[1]))
for (i in 1:dim(url_list)[1]){
wiki_list <-  url_list$link[i] %>%
read_html() %>%
html_node(class_of_interest) %>%
html_children() %>%
map(., list()) %>%
tibble(node = .) %>%
mutate(type = map_chr(node, html_name)) %>%
filter(type == "p") %>%
mutate(text = map_chr(node, html_text)) %>%
mutate(cleantext = str_remove_all(text, "\\[.*?\\]") %>% str_trim()) %>%
plyr::summarise(cleantext = paste(cleantext, collapse = "\n"))
wiki_pages$page_notes[i] <- wiki_list$cleantext[1]
}
i
url_list$link[i]
class_of_interest <- ".mw-content-ltr" ## ids are #id-name, classes are .class-name
editurl <- "https://en.wikipedia.org/w/index.php?title=Highlighter&action=history&offset=&limit=150"
editclass_of_interest <- ".mw-changeslist-date"
url_list <- editurl %>%
read_html() %>%
html_nodes(editclass_of_interest) %>%
map(., list()) %>%
tibble(node = .) %>%
mutate(link = map_chr(node, html_attr, "href") %>% paste0("https://en.wikipedia.org", .))
wiki_pages <- data.frame(page_notes = rep(NA, dim(url_list)[1]))
for (i in 1:dim(url_list)[1]){
wiki_list <-  url_list$link[i] %>%
read_html() %>%
html_node(class_of_interest) %>%
html_children() %>%
map(., list()) %>%
tibble(node = .) %>%
mutate(type = map_chr(node, html_name)) %>%
filter(type == "p") %>%
mutate(text = map_chr(node, html_text)) %>%
mutate(cleantext = str_remove_all(text, "\\[.*?\\]") %>% str_trim()) %>%
plyr::summarise(cleantext = paste(cleantext, collapse = "\n"))
wiki_pages$page_notes[i] <- wiki_list$cleantext[1]
}
usethis::use_data(wiki_pages, overwrite = TRUE)
devtools::install()
url_list
class_of_interest <- ".mw-content-ltr" ## ids are #id-name, classes are .class-name
editurl <- "https://en.wikipedia.org/w/index.php?title=Highlighter&action=history&offset=&limit=150"
editclass_of_interest <- ".mw-changeslist-date"
url_list1 <- editurl %>%
read_html() %>%
html_nodes(editclass_of_interest) %>%
map(., list()) %>%
tibble(node = .) %>%
mutate(link = map_chr(node, html_attr, "href") %>% paste0("https://en.wikipedia.org", .))
editurl2 <- "https://en.wikipedia.org/w/index.php?title=Highlighter&action=history&dir=prev&limit=150"
url_list2 <- editurl2 %>%
read_html() %>%
html_nodes(editclass_of_interest) %>%
map(., list()) %>%
tibble(node = .) %>%
mutate(link = map_chr(node, html_attr, "href") %>% paste0("https://en.wikipedia.org", .))
rbind(url_list1, url_list2)
url_list1
tail(rbind(url_list1, url_list2))
tail(url_list2)
url_list <- rbind(url_list1, url_list2)
wiki_pages <- data.frame(page_notes = rep(NA, dim(url_list)[1]))
for (i in 1:dim(url_list)[1]){
wiki_list <-  url_list$link[i] %>%
read_html() %>%
html_node(class_of_interest) %>%
html_children() %>%
map(., list()) %>%
tibble(node = .) %>%
mutate(type = map_chr(node, html_name)) %>%
filter(type == "p") %>%
mutate(text = map_chr(node, html_text)) %>%
mutate(cleantext = str_remove_all(text, "\\[.*?\\]") %>% str_trim()) %>%
plyr::summarise(cleantext = paste(cleantext, collapse = "\n"))
wiki_pages$page_notes[i] <- wiki_list$cleantext[1]
}
usethis::use_data(wiki_pages, overwrite = TRUE)
devtools::install()
devtools::build_rmd("vignettes/wikipedia-highlighter.Rmd")
devtools::build_rmd("vignettes/wikipedia-highlighter.Rmd")
devtools::install()
devtools::build_rmd("vignettes/wikipedia-highlighter.Rmd")
round(4.6)
devtools::install()
devtools::install()
devtools::install()
colors=c("#f251fc","#f8ff1b")
colors[1]
devtools::install()
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(highlightr)
toks_comment <- token_comments(highlightr::wiki_pages)
toks_comment <- token_comments(wiki_pages)
remove.packages("highlightr")
devtools::install()
devtools::install()
devtools::install()
class_of_interest <- ".mw-content-ltr" ## ids are #id-name, classes are .class-name
editurl <- "https://en.wikipedia.org/w/index.php?title=Highlighter&action=history&offset=&limit=150"
editclass_of_interest <- ".mw-changeslist-date"
url_list1 <- editurl %>%
read_html() %>%
html_nodes(editclass_of_interest) %>%
map(., list()) %>%
tibble(node = .) %>%
mutate(link = map_chr(node, html_attr, "href") %>% paste0("https://en.wikipedia.org", .))
library(tidyverse)
url_list1 <- editurl %>%
read_html() %>%
html_nodes(editclass_of_interest) %>%
map(., list()) %>%
tibble(node = .) %>%
mutate(link = map_chr(node, html_attr, "href") %>% paste0("https://en.wikipedia.org", .))
library(httr)
library(rvest)
library(magrittr)
library(purrr)
library(dplyr)
library(stringr)
class_of_interest <- ".mw-content-ltr" ## ids are #id-name, classes are .class-name
editurl <- "https://en.wikipedia.org/w/index.php?title=Highlighter&action=history&offset=&limit=150"
editclass_of_interest <- ".mw-changeslist-date"
url_list1 <- editurl %>%
read_html() %>%
html_nodes(editclass_of_interest) %>%
map(., list()) %>%
tibble(node = .) %>%
mutate(link = map_chr(node, html_attr, "href") %>% paste0("https://en.wikipedia.org", .))
editurl2 <- "https://en.wikipedia.org/w/index.php?title=Highlighter&action=history&dir=prev&limit=150"
url_list2 <- editurl2 %>%
read_html() %>%
html_nodes(editclass_of_interest) %>%
map(., list()) %>%
tibble(node = .) %>%
mutate(link = map_chr(node, html_attr, "href") %>% paste0("https://en.wikipedia.org", .))
url_list <- rbind(url_list1, url_list2)
wiki_pages <- data.frame(page_notes = rep(NA, dim(url_list)[1]))
for (i in 1:dim(url_list)[1]){
wiki_list <-  url_list$link[i] %>%
read_html() %>%
html_node(class_of_interest) %>%
html_children() %>%
map(., list()) %>%
tibble(node = .) %>%
mutate(type = map_chr(node, html_name)) %>%
filter(type == "p") %>%
mutate(text = map_chr(node, html_text)) %>%
mutate(cleantext = str_remove_all(text, "\\[.*?\\]") %>% str_trim()) %>%
plyr::summarise(cleantext = paste(cleantext, collapse = "<br> "))
wiki_pages$page_notes[i] <- wiki_list$cleantext[1]
}
usethis::use_data(wiki_pages, overwrite = TRUE)
devtools::install()
devtools::install()
toks_comment <- token_comments(highlightr::wiki_pages)
library(highlightr)
toks_comment <- token_comments(highlightr::wiki_pages)
toks_comment
toks_comment[[1]]
head(wiki_pages)
transcript_example_rename <- data.frame(text=wiki_pages[1,])
toks_transcript <- token_transcript(transcript_example_rename)
toks_transcript
toks_transcript[[1]]
collocation_object <- collocate_comments_fuzzy(toks_transcript, toks_comment)
head(collocation_object)
View(collocation_object)
merged_frequency <- transcript_frequency(transcript_example_rename, collocation_object)
View(merged_frequency)
testing <- c("thwelif<br>", "ltwheiohdf", "lwfhso")
gsub("<br>","",testing)
transcript<- transcript_example_rename
collocate_object <- collocation_object
# Code from Dr. Vanderplas' 850 class
poem <- transcript %>% tibble::tibble(lines = text) %>%
# This looks for a letter + a space (of any sort, so an end-line counts) or
# punctuation (last word of a line ends with e.g. a period or comma)
dplyr::mutate(n_words = stringr::str_count(lines, "([A-z][[:space:][:punct:]])"))
`%>%` <- magrittr::`%>%`
# Code from Dr. Vanderplas' 850 class
poem <- transcript %>% tibble::tibble(lines = text) %>%
# This looks for a letter + a space (of any sort, so an end-line counts) or
# punctuation (last word of a line ends with e.g. a period or comma)
dplyr::mutate(n_words = stringr::str_count(lines, "([A-z][[:space:][:punct:]])"))
poem
poem[1]
poem[[1]]
poem$lines <- gsub("/"," ",  poem$lines)
poem_words <- poem %>%
dplyr::mutate(words = stringr::str_split(lines, "[[:space:]]", simplify = F)) %>%
tidyr::unnest(c(words)) %>%
# Require words to have some non-space character
dplyr::filter(nchar(stringr::str_trim(words)) > 0) %>%
# dplyr::filter(!(words %in% c("<br", "><br", ">"))) %>%
dplyr::mutate(word_num = 1:dplyr::n())
poem_words
View(poem_words)
# counting the number of characters
poem_words$word_length<-nchar(poem_words$words)
poem_words$x_coord <- NA
# Assigning coordinates to be used for plotting
for (i in 1:length(poem_words$n_words)){
if (poem_words$word_num[i] == 1){
poem_words$x_coord[i] = 1
}
else {
poem_words$x_coord[i] = poem_words$x_coord[i-1]+poem_words$word_length[i-1]*2
}
}
View(poem_words)
poem_words$to_merge<- tolower(poem_words$words)
poem_words$to_merge<- tm::removePunctuation(poem_words$to_merge)
poem_words$to_merge<- gsub("<br>","", poem_words$to_merge)
View(poem_words)
poem_words$to_merge<- tolower(poem_words$words)
poem_words$to_merge<- gsub("<br>","", poem_words$to_merge)
View(poem_words)
devtools::install()
library(highlightr)
toks_comment <- token_comments(highlightr::wiki_pages)
transcript_example_rename <- data.frame(text=wiki_pages[1,])
toks_transcript <- token_transcript(transcript_example_rename)
collocation_object <- collocate_comments_fuzzy(toks_transcript, toks_comment)
collocation_object <- collocate_comments_fuzzy(toks_transcript, toks_comment)
head(collocation_object)
merged_frequency <- transcript_frequency(transcript_example_rename, collocation_object)
poem$lines <- gsub("<.*?>", "<.>", poem$lines)
poem$lines <- gsub("<.*?>", "<^>", poem$lines)
object <- c("<br>", "< br>")
gsub("<.*?>", "<^>", object)
gsub("<.*?>", "<[^ ]>", object)
grepl("<.*?>", object)
str_locate_all(object, "<.*?>")
gregexpr(object, "<.*?>")
gregexpr(object[1], "<.*?>")
str_extract(object, "<.*?>")
library(stringr)
str_extract(object, "<.*?>")
use_package("stringr")
library(devtools)
use_package("stringr")
gsub(">", "> ", object)
object <- c("<br><br>", "<br>t", "<br> ")
gsub(">", "> ", object)
gsub(">[^ ]", "> ", object)
gsub(">[^ <]", "> ", object)
gsub(">[^ <]", "\\1 ", object)
gsub("(>)([^ <])", "\\1 ", object)
object
gsub("(>)([^ <])", "\\1 \\2", object)
object <- c("<br><br>y", "<br>t", "<br> ")
gsub("(>)([^ <])", "\\1 \\2", object)
library(devtools)
document()
